---
title: ðŸ“Ÿ Qubit rotation using gradient descent. QML01
author: Rohith Krishna
date: 11 September 2020
layout: post
permalink: /2020-09-11-qubit-rotation-gradient-descent
tags: [quantum machine learning, qiskit, pennylane.ai, gradient descent, python]
typora-root-url: ../../rohithmay.github.io
---

## Where can one apply quantum machine learning algorithms?

Some applications of quantum machine learning are enumerated:

1. Logistics. In logistics, there are problems that involve the interaction of several variables - like transportations systems. Quantum computers and quantum algorithms are particularly useful in solving complex combinatorial problems. 
2. Finance. Quantum Computing has already shown to have quadratic speed-ups in Monte Carlo methods that come up in pricing assets and derivatives, risk management, portfolio management and optimization etc., with much scope for innovation. 
3. Material Science. The study of molecules is a very difficult problem with our current hardware and the hope is that quantum computers can handle these difficult computations and simulations in quantum chemistry - with applications in drug design, protein synthesis etc. 
4. The Universe! - Richard Feynman famously quipped that nature is quantum and that any  successful model of nature must incorporate quantum effects, which quantum computing essentially involves. 

> â€œ**Nature** isn't classical, dammit, and if you want to make a simulation of **nature**, you'd better make it **quantum** mechanical, and by golly it's a wonderful problem, because it doesn't look so easy.â€ 									-- Richard Feynman

## Quantum Machine Learning

Researchers often cast quantum and classical computing methods into the following four blocks, based on the classical/quantum nature of the data generating system and the data processing device. 

![2020-09-12-qubit-rotation-gradient-descent-img01](/images/2020-09-12-qubit-rotation-gradient-descent-img01.jpg)

> Source: Schuld, Maria. *Supervised learning with quantum computers*. Springer, 2018.

- **Classical Classical (CC)**. This would typically correspond to classical data used in classical processing devices and would fall under standard supervised and unsupervised machine learning model. Using quantum-theory inspired ML models also fall under this category.
- **Quantum Classical (QC)**. Here machine learning methods are used to derive insights on quantum measurement data, or in learning phase-transitions in many-body systems, etc. Several experimental and computational data generated by quantum processes can be studied using classical devices. 
- **Classical Quantum (CQ)**. CQ and QQ are used synonymously with *quantum machine learning*. In CQ, one uses classical data such as time series variables or sales data or text or images and uses specially designed quantum algorithms to solve ML problems. These could either be supervised learning problems or even unsupervised learning methods that perform computationally complex tasks with ease. 
- **Quantum Quantum (QQ)**. In physics and chemistry, one often wishes to simulate dynamics of a system that is inherently quantum mechanical in nature. When quantum computers are used in the generation/simulation of quantum mechanical system we refer to it as QQ.

### Quantum Machine Learning Models

We know from *(classical)* Machine Learning that a parameterized model can be represented by $$ f (x;\theta)$$. Here, $$x$$ is  some data that is input to the model and $$\theta$$ represents the model's parameters. The objective is to choose optimal parameters $$\theta$$ such that this model gives useful insights such as predictions of a response variable or predicted class of a target variable etc. Quantum Machine learning in some sense translates this process into a quantum system and the model output would be a quantum state with probabilities of occurence and expectation values, and one would have to make a measurement to obtain an output. 

$$ f(x; \theta) \mapsto \ket{f(x; \theta)} $$

The idea is that this Quantum Machine Learning (QML) model, which we are yet to define, has similar objectives as a CML model, but uses quantum mechanical algorithms which inherently require a measurement process in some basis to obtain an output. It is pertinent now to ask whether 1. such a thing can be done? and if yes, 2. Is there an advantage in using QML models over standard ML models?

QML models are broadly of two kinds: *deterministic* and *variational* quantum models. The Deutsch-Jorza algorithms is an example for a **deterministic quantum model**. Here, the state $$\ket{\psi}$$ is transformed by circuits represented by $$\boxed{U}$$ to obtain the output $$y$$, with certainty. 

$$ \ket{\psi} \longrightarrow \boxed{U} \longrightarrow y $$

The other class that is popular in QML is the **variational quantum models**, which are applied in quantum chemistry amongst other fields. Examples for this model include: quantum variational eigensolver, quantum classifier, quantum support vector machines and quantum neural network. The basic premise of the variational model is that while it inputs a state $$\ket{\psi}$$, the output we obtain is a probability distribution over the possible outcomes, and one resorts to taking expectation values for the same $$\langle y \rangle $$. Also, note: the circuits $$Â \boxed{U(\theta)}$$ depend on model-specific parameters $$\theta$$. 

$$ \ket{\psi} \longrightarrow \boxed{U(\theta)} \longrightarrow \langle y \rangle $$

### Variational quantum models

One often finds several applications for the variational model, especially in the fields of chemistry and finance. This is because these variational models work really well. They run on near-term quantum hardware and are robust to noise in the system. Our simple circuit above can be extended when there are several input qubits, depicted below. Several qubits ($$n$$) are tranformed by the operations in $$\boxed{U}$$ and we obtain a function which is some kind of encoding of the data and parameters - $$ \ket{f(x;\theta)}$$. The difference therefore, is that, since the outputs are probability distributions (or a superposition of output states), one has to perform a **measurement** in a certain basis to extract a real output, which in the ML parlance would be a predictor $$\hat{y}$$. The act of observing the state or measurement is represented by $$\boxed{M}$$. Note that circuits with classical input/output are represented by single lines $$ (\rightarrow)$$, while those which represent superposition of  states are represented by double lines $$ (\Rightarrow)$$. Hence, 

$$ \ket{\psi}^{\bigotimes n}  \longrightarrow \boxed{U(x;\theta)} \longrightarrow   \boxed{M} \Longrightarrow   \hat{y} $$

#### A note on measurement

Quantum mechanics introduces the method of calculating expectation values for states during measurement; this is discussed briefly in this section. Consider the pure state $$\ket{0}$$. In matrix notation, this is the column vector, $$ \ket{0} = [1, 0]^T$$. In the Bloch sphere, this would be along the $$z$$ direction, and a measurement in $$\sigma_z$$ would give a value of 1.  

$$  \ket{0} \longrightarrow \boxed{M} \  _{ \sigma_z} \longrightarrow 1 $$

$$ \bra{0} \sigma_z \ket{0} = \begin{bmatrix} 1 &  0 \\  \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix}  = 1  $$ 

Likewise, one can apply the $$X$$ gate to rotate $$\ket{0}$$ by 180 degrees in the Bloch sphere to get the vector $$ \ket{1}$$ in the $$-z$$ direction. $$ X$$ is the state flip gate. Note however that these vectors are orthogonal in Hilbert space. A measurement of $$\ket{1}$$ in $$ \sigma_z$$ would result in -1. Therefore, the expectation value of output from the variational circuit $$ \langle \sigma_z \rangle $$ ranges from +1 to -1. 

$$  \ket{0} \longrightarrow \boxed{X} \longrightarrow \ket{1} \longrightarrow \boxed{M} \  _{ \sigma_z} \longrightarrow -1 $$

$$ X \bra{0} = \begin{bmatrix} 0 & 1 \\ 1& 0  \end{bmatrix} \begin{bmatrix} 1 \\0  \end{bmatrix} =  \begin{bmatrix} 0 \\ 1  \end{bmatrix} = \ket{1} $$ 

$$ \bra{1} \sigma_z \ket{1} = \begin{bmatrix} 0 &  1 \\  \end{bmatrix} \begin{bmatrix} 1 & 0 \\ 0 & -1 \end{bmatrix} \begin{bmatrix} 0 \\ 1 \end{bmatrix}  = -1  $$ 

![2020-09-12-qubit-rotation-gradient-descent-img02](/images/2020-09-12-qubit-rotation-gradient-descent-img02.jpg)

> A visualization of the orthogonal $$\ket{0}$$ and $$\ket{1}$$ pure states in the Bloch sphere. 



### A simple variational circuit

Let us now get back to our model and perform the following parametrized operations. Take the $$ \ket{0} $$ state and rotate this vector through $$x$$  axis by an angle of $$\theta_1$$ and then the $$y$$ axis by $$ \theta_2$$ using the rotation matrices $$R_x(\theta_1)$$ and $$R_y(\theta_2)$$, consecutively. Suppose the state of the system after performing these operations is $$\ket{\psi}$$. Now, perform a $$\sigma_z$$ measurement to obtain the its expected value. We can easily compute this value in steps shown below:

$$ \ket{0} \longrightarrow \boxed{R_x(\theta_1)} \longrightarrow \boxed{R_y(\theta_2) }  \longrightarrow  \ket{\psi} \longrightarrow \boxed{M} \ _{\sigma_z} \longrightarrow \langle \sigma_z \rangle $$

Note the definitions of the rotation matrices:

$$  R_x(\theta_1) = \begin{bmatrix} \cos{\frac{\theta_1}{2}}  & - i \sin{\frac{\theta_1}{2}}  \\\ - i \sin{\frac{\theta_1}{2}}    & \cos{\frac{\theta_1}{2}}   \end{bmatrix}, \ \ \ \ \  R_y(\theta_2) = \begin{bmatrix} \cos{\frac{\theta_2}{2}}  & - \sin{\frac{\theta_2}{2}}  \\\  \sin{\frac{\theta_2}{2}}   & \cos{\frac{\theta_2}{2}}   \end{bmatrix} $$

The state of the system after rotations: $$\ket{\psi}$$ is measured along the $$z$$ direction as:

$$ \bra{\psi} \sigma_z \ket{\psi} = \bra{0} \ R_x^\dagger(\theta_1)  \ R_y^\dagger(\theta_2)  \ \sigma_z  \ R_y(\theta_2) \  R_x(\theta_1) \ \ket{0} = \cos{\theta_1} \cos{\theta_2}$$

Now that we have computed the expected value of this variational circuit to be $$ \bra{\psi} \sigma_z \ket{\psi} = \cos{\theta_1} \cos{\theta_2}$$, we can now think about choosing these parameters: $$\theta_1 $$ and $$\theta_2$$.

### Circuit Optimization

In classical Machine Learning, one defines a cost function for the chosen model, using techniques that minimize the cost function, one obtains the optimal values for the parameters. For example, a decision trees model has a variety of hyperparameters such as maximum depth of trees, number of child nodes at a particular parent node etc.; a random forest method involves optimizing the out-of-bag score, again using several hyperparameters. In neural networks, weights are used for the same purpose. The general scheme of optimization for the variational model would be:

![2020-09-12-qubit-rotation-gradient-descent-img03](/images/2020-09-12-qubit-rotation-gradient-descent-img03.jpg)

> Cost function minimisation used in ML to find optimal model parameters.





 However, one wonders now about the analogue of this in the quantum setting. 



